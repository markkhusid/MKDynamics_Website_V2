# CSE571 AI Project 1 - Neural Network for Collision Prediction

```{note}
Project was written in Python and summarized here by Grok 4.1
```

```{admonition} Note
This document provides a comprehensive explanation of the provided project files, which appear to implement a robot simulation environment for collecting training data using steering behaviors. The project uses Python libraries like Pymunk for physics simulation, Pygame for rendering (optionally headless), and Perlin noise for generating wandering actions. The goal seems to be generating a dataset of sensor readings, actions, and collision outcomes for training a machine learning model (e.g., for collision avoidance).
```

## Project Overview

This project simulates a robot navigating a walled environment with sensors to detect obstacles. The robot uses a "wander" steering behavior to move randomly while collecting data on its sensor readings, chosen actions, and whether collisions occur. The simulation is built on Pymunk for physics and collision detection, with optional visualization via Pygame. The collected data is saved as CSV files (e.g., `training_data_20k.csv` and `training_data_10k.csv`), where each row represents a sample with 5 sensor values, an action integer, and a collision flag (0 or 1).

Key components:
- **Simulation Environment**: Handles physics, robot movement, sensors, walls, and a goal.
- **Steering Behaviors**: Implements random wandering (using Perlin noise) and seeking (towards a target).
- **Data Collection**: Runs the simulation for a specified number of actions, records sensor data and outcomes.
- **Helper Functions**: Math utilities for angles, vectors, and conversions.
- **Datasets**: Pre-collected CSV files with truncated content shown.

The project is designed to run in a headless mode (no GUI) for efficiency during data collection, but can render visuals if needed.

## File Breakdown

### 1. Datasets: `training_data_20k.csv` and `training_data_10k.csv`

These are CSV files containing the training data generated by the simulation. Each row is a sample with 7 columns (no headers):
- Columns 1-5: Sensor readings (distances to obstacles from 5 sensors, in pixels; max 150.0 indicates no detection within range).
- Column 6: Action (integer from -5 to 5, representing steering direction).
- Column 7: Collision flag (0 for no collision, 1 for collision during the action).

Example rows from `training_data_20k.csv` (truncated for brevity):
```
150.0,150.0,150.0,150.0,150.0,0,0
150.0,150.0,150.0,150.0,150.0,-2,0
150.0,150.0,150.0,150.0,150.0,1,0
...
```

The data shows many "clear" readings (150.0) with varying actions and occasional collisions. These files are used as outputs of the data collection script.

```{admonition} Insight
The sensor readings are raycast distances. Values <150 indicate proximity to walls. Actions are discrete (-5 to 5), scaled by a wander range (0.1 * π radians) to compute steering forces.
```

### 2. `SimulationEnvironment.py`

This file defines the core simulation using Pymunk (physics engine) and Pygame (rendering).

#### Key Classes:
- **Robot**: Represents the robot with physics body, shape, and sensors.
  - Mass, speed, steering limits, and friction are defined.
  - Sensors: 5 raycast segments at angles [66°, 33°, 0°, -33°, -66°] relative to the robot's orientation, with a range of 150 units.
  - Pymunk body is a rectangular box; sensors are line segments.

- **SimulationEnvironment**: Manages the world, robot, walls, and goal.
  - Initializes Pygame screen (1080x900) and Pymunk space.
  - Adds walls (outer boundaries and inner obstacles).
  - Adds a circular goal that randomly moves to one of 6 positions.
  - Handles collisions with walls (resets robot on impact).
  - Raycasting for sensor readings (computes distances to nearest obstacles).
  - Steps the simulation: Applies forces, checks collisions, updates physics.

#### Important Methods:
- **`step(steering_direction)`**: Applies steering force, friction, updates physics, detects collisions, and returns state (position, orientation), collision flag, and sensor readings.
- **`raycasting()`**: Computes sensor distances using Pymunk segment queries.
- **`_draw_everything()`**: Renders the scene (robot, sensors, walls, goal) if not headless.
- **`_apply_robot_motion()`**: Applies steering and friction forces; aligns orientation with velocity.
- **`_detect_collisions()`**: Checks for robot-wall overlaps; flashes damage if collided.
- **`turn_robot_around()`**: Reverses direction on prolonged collisions.
- **`move_goal()`**: Relocates the goal to avoid overlap with walls.

The environment runs at 50 FPS (space.step(1/50.0)) and supports headless mode by setting `HEADLESS = True` and using a dummy video driver.

```{admonition} Technical Note
Pymunk uses categories and filters for collision groups (e.g., robot in category 0b1). Sensors are marked as `sensor=True` to not affect physics but detect overlaps.
```

#### Coordinate Conversions:
- `pm2pgP` and `pm2pgV`: Convert Pymunk to Pygame coordinates (flips Y-axis if needed, but in this code, it's identity).
- Angles use radians internally; degrees for Pygame rotations.

### 3. `SteeringBehaviors.py`

Defines autonomous steering behaviors for the robot.

#### Classes:
- **Wander**: Random movement using Perlin noise.
  - Parameters: Action repeat (how long an action persists), wander range (0.1π), max scaler (5 for actions -5 to 5).
  - Uses two Perlin noise sources for smooth randomness.
  - **`get_action(timestep_i, current_orientation)`**: Computes action integer via noise; ensures uniqueness if checked against prior actions.
  - **`get_steering_force(action, current_orientation)`**: Converts action to a vector force.
  - **`reset_action()`**: Randomizes noise offsets on collision.

- **Seek**: Goal-directed movement (not used in data collection but available).
  - Targets a position; computes the closest discrete action aligning with the direction to the target.
  - Updates goal dynamically.

These behaviors output discrete actions (-5 to 5) and continuous force vectors for the simulation.

```{admonition} Why Perlin Noise?
Perlin noise provides smooth, natural-looking randomness over time, ideal for simulating wandering without abrupt changes.
```

### 4. `collect_data.py`

The main script for running the simulation and collecting data.

- Initializes `SimulationEnvironment`.
- Uses `Wander` behavior with action_repeat=100 (each action lasts 100 simulation steps).
- Loops for `total_actions` (e.g., 20,000):
  - Gets action and steering force.
  - Applies the force for up to 100 steps; breaks early on collision.
  - Records sensor readings (from first step), action, and collision.
  - Resets wander on collision; shares collision with prior action if early in the repeat.
- Saves data to CSV (e.g., `training_data_20k.csv`).

This script prints progress and individual samples during collection.

```{admonition} Customization Tip
Students are instructed to edit only the data appending and CSV saving parts. The loop structure ensures data is collected over many random trajectories.
```

### 5. `Helper.py`

Utility functions for math:
- `radians(degrees)` and `degrees(radians)`: Angle conversions (note: degrees flips sign for Pygame compatibility).
- `angle(vector)`: Arctan2 for vector to angle.
- `vector(angle)`: Cos/sin for angle to unit vector.
- Constants: PI, PIx2, GRAVITY.

Imports Pymunk options to disable debug printing.

## How the Simulation Works

1. **Setup**: Environment creates space, robot at center facing east, walls, and goal.
2. **Action Loop** (in `collect_data.py`):
   - Wander computes a noisy action (-5 to 5).
   - For 100 steps: Apply steering force, step physics, check collision.
   - If collision: Reset robot, mark data, reset wander.
3. **Sensors**: Raycast from robot in 5 directions; distances capped at 150.
4. **Movement**: Force-based; friction slows robot; orientation aligns with velocity.
5. **Rendering**: Optional; draws robot image (rotated), lines for velocity/steering, colored sensors (red on hit).
6. **Goal**: Moves randomly but unused in data collection (perhaps for future seeking tasks).

Collisions trigger a brief pause and visual feedback if not headless.

## Data Collection Process

- **Purpose**: Generate supervised data for training (e.g., predict safe actions from sensors).
- **Samples**: 20k or 10k actions, each with 100 sub-steps (but data from first step only).
- **Collisions**: Rare (mostly 0), occur when sensors fail to prevent wall hits.
- **Truncation**: CSV snippets show partial data; full files would have ~20k/10k rows.

## Potential Extensions

- Train a neural network on the CSV to predict actions or collisions.
- Use Seek behavior for goal-reaching data.
- Add more sensors or complex environments.

## Conclusion

This project provides a solid foundation for simulating autonomous robot navigation and collecting behavioral data. The use of physics-based simulation ensures realistic interactions, while Perlin noise adds variability to wandering paths. The datasets can be directly used for machine learning tasks like reinforcement learning or supervised classification for obstacle avoidance.

For running: Execute `collect_data.py` to generate new data. Set `HEADLESS=False` in `SimulationEnvironment.py` for visuals.