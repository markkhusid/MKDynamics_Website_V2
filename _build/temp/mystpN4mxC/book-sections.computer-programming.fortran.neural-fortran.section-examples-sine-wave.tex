\begin{verbatim}
- - -
jupytext:
  formats: md:myst
  text_representation:
    extension: .md
    format_name: myst
kernelspec:
  display_name: Python 3
  language: python
  name: python3
 - - -
\end{verbatim}

\subparagraph{Neural Fortran Examples: Sine Wave}

\subparagraph{Introduction}

In this example, we will train a neural network to approximate the sine function. We will use a simple feedforward neural network with two hidden layers. The network will be trained using the backpropagation algorithm.  The data will be written to a CSV file using the CSV Fortran library \href{https://github.com/jacobwilliams/csv-fortran}{https://github.com/jacobwilliams/csv-fortran}. The neural network will be implemented in Fortran using the Neural Fortran library \href{https://github.com/modern-fortran/neural-fortran}{https://github.com/modern-fortran/neural-fortran}.

Once the data is collected and saved to a CSV file, Pandas and Matplotlib will be used to plot the learned sine wave and mean squared error versus the learning iteration number.

\subparagraph{Neural Network Theory by ChatGPT o1}

Below is a concise yet mathematically grounded overview of \textbf{dense neural networks}, sometimes referred to as \textbf{feedforward} or \textbf{fully connected} networks. We will cover their core structure, forward-pass equations, and typical training approach.


\bigskip
\centerline{\rule{13cm}{0.4pt}}
\bigskip

\subparagraph{Dense Neural Network Structure}

A \textbf{dense (fully connected) neural network} consists of layers of neurons, where each neuron in one layer connects to \textbf{every} neuron in the next layer via a trainable weight. Suppose we have a network with $L$ layers, indexed by $\ell = 1, 2, \ldots, L$. Each layer $\ell$ has $n_\ell$ neurons. For simplicity, let $\mathbf{x} \in \mathbb{R}^{n_0}$ be the input, where $n_0$ is the input dimension, and let $\mathbf{y} \in \mathbb{R}^{n_L}$ be the network's output.

A typical layer $\ell$ of the network (for $1 \le \ell \le L$) is described by a linear transformation followed by an elementwise non-linear activation. Specifically,

\begin{equation}
\mathbf{z}^{(\ell)} 
  \;=\; W^{(\ell)}\, \mathbf{h}^{(\ell-1)} \;+\; \mathbf{b}^{(\ell)},
\end{equation}

where:

\begin{itemize}
\item $W^{(\ell)} \in \mathbb{R}^{n_\ell \times n_{\ell -1}}$ is the weight matrix for layer $\ell$.
\item $\mathbf{b}^{(\ell)} \in \mathbb{R}^{n_\ell}$ is the bias vector.
\item $\mathbf{h}^{(\ell -1)} \in \mathbb{R}^{n_{\ell -1}}$ is the output (also called the activation) of the previous layer ($\ell -1$).
\end{itemize}

Next, we apply a (usually nonlinear) activation function $\sigma(\cdot)$ elementwise:

\begin{equation}
\mathbf{h}^{(\ell)} 
  \;=\; \sigma\bigl(\mathbf{z}^{(\ell)}\bigr)
  \;=\; 
  \begin{bmatrix}
    \sigma(z_1^{(\ell)}) \\[6pt]
    \sigma(z_2^{(\ell)}) \\[6pt]
    \vdots \\[6pt]
    \sigma(z_{n_\ell}^{(\ell)}) \\[6pt]
  \end{bmatrix}.
\end{equation}

For instance, $\sigma$ might be the ReLU function $\max(0, z)$, the sigmoid function $\tfrac{1}{1 + e^{ -z}}$, or $\tanh$. The final output layer might also have a special activation (e.g., softmax for classification).


\bigskip
\centerline{\rule{13cm}{0.4pt}}
\bigskip

\subparagraph{Forward Pass Equations}

Using the above notation, we initialize the network's input as

\begin{equation}
\mathbf{h}^{(0)} \;=\; \mathbf{x}.
\end{equation}

Then for each layer $\ell = 1, 2, \ldots, L$, we compute

\begin{equation}
\mathbf{z}^{(\ell)} = W^{(\ell)}\, \mathbf{h}^{(\ell-1)} + \mathbf{b}^{(\ell)}, 
  \quad
  \mathbf{h}^{(\ell)} = \sigma\bigl(\mathbf{z}^{(\ell)}\bigr).
\end{equation}

At the final layer, $\mathbf{h}^{(L)}$ becomes the network's output $\mathbf{y}$. Hence, a forward pass through a dense network is just iterating these matrix-vector multiplications and elementwise activations.


\bigskip
\centerline{\rule{13cm}{0.4pt}}
\bigskip

\subparagraph{Training With Gradient Descent}

To train the network, we typically have a dataset of inputs $\{\mathbf{x}_i\}$ and corresponding targets $\{\mathbf{t}_i\}$. We define a \textbf{loss function} $\mathcal{L}$, such as mean squared error (for regression) or cross-entropy (for classification). For a single training example $(\mathbf{x}, \mathbf{t})$, we might write

\begin{equation}
\mathcal{L}(\mathbf{t}, \mathbf{y}) 
  \;=\; \frac{1}{2} \|\mathbf{y} - \mathbf{t}\|^2
  \quad\text{(for an example squared error case).}
\end{equation}

The \textbf{backpropagation} algorithm computes partial derivatives of $\mathcal{L}$ with respect to each network parameter $\{W^{(\ell)}, \mathbf{b}^{(\ell)}\}$. Gradient-based optimizers (e.g., stochastic gradient descent) then update the parameters in the direction that reduces the loss:

\begin{equation}
W^{(\ell)} \;\leftarrow\; W^{(\ell)} \;-\; \eta \,\frac{\partial \mathcal{L}}{\partial W^{(\ell)}}
  \quad,\quad
  \mathbf{b}^{(\ell)} \;\leftarrow\; \mathbf{b}^{(\ell)} \;-\; \eta \,\frac{\partial \mathcal{L}}{\partial \mathbf{b}^{(\ell)}},
\end{equation}

where $\eta$ is the learning rate. Repeated over many iterations (epochs), the network learns weights that (hopefully) minimize the loss on the training set and generalize to unseen data.


\bigskip
\centerline{\rule{13cm}{0.4pt}}
\bigskip

This dense architecture is the foundational design for many neural networks. While modern advances (like convolutional or transformer layers) may structure connections differently, the core linear+activation principle remains central to deep learning.

\subparagraph{Code Analysis by ChatGTP o1}

Below is a detailed walkthrough of how the \textbf{\texttt{program sine}} works, focusing on what each block of code accomplishes and how it fits into the overall workflow of training a sine-wave--inspired neural network. We will cover:

\begin{enumerate}
\item \textbf{Imports and Declarations}
\item \textbf{CSV Logging Initialization}
\item \textbf{Creating Test Data}
\item \textbf{Defining the Neural Network}
\item \textbf{Training Loop} (Forward \& Backward passes, updating weights)
\item \textbf{Computing Mean Squared Error}
\item \textbf{Logging Data to CSV}
\end{enumerate}

Throughout, note that the network code comes from a library \texttt{nf} (with \texttt{dense}, \texttt{input}, \texttt{network}, \texttt{sgd}), and CSV output is handled by \texttt{csv\_module}. The code also uses single-precision real numbers (\texttt{real32}) for training variables and a double-precision \texttt{pi} in some places.


\bigskip
\centerline{\rule{13cm}{0.4pt}}
\bigskip

\subparagraph{1. \textbf{Imports and Declarations}}

\begin{verbatim}
program sine
  use csv_module
  use nf, only: dense, input, network, sgd
  use iso_fortran_env, only: real32

  implicit none

  real, parameter :: pi = 4 * atan(1.)
\end{verbatim}

\begin{enumerate}
\item \textbf{\texttt{csv\_module}}: A module that provides CSV reading/writing functionality.
\item \textbf{\texttt{nf}}: A custom or third-party neural network framework that exposes:

\begin{itemize}
\item \texttt{dense}: A fully connected (dense) layer constructor.
\item \texttt{input}: An input layer constructor.
\item \texttt{network}: A type or class that composes layers and provides \texttt{forward}, \texttt{backward}, \texttt{update}, \texttt{predict}.
\item \texttt{sgd}: A function or type for the Stochastic Gradient Descent optimizer.
\end{itemize}


\item \textbf{\texttt{iso\_fortran\_env, only: real32}}: Imports the \texttt{real32} kind for single-precision floats (32-bit).
\item \textbf{\texttt{implicit none}}: Requires explicit declarations for all variables.
\item \textbf{\texttt{pi}}: Defined via \texttt{4 * atan(1.)}, which yields $\pi \approx 3.14159265...$.
\end{enumerate}


\bigskip
\centerline{\rule{13cm}{0.4pt}}
\bigskip

\subparagraph{2. \textbf{CSV Logging Initialization}}

\begin{verbatim}
type(csv_file) :: sine_NN_data_file
  type(network) :: net

  logical :: status_ok
\end{verbatim}

\begin{itemize}
\item \textbf{\texttt{sine\_NN\_data\_file}}: A \texttt{csv\_file} object that will handle CSV writing.
\item \textbf{\texttt{net}}: A neural network object.
\end{itemize}

\begin{verbatim}
integer, parameter :: num_iterations = 100000
  integer, parameter :: test_size = 50
  real(kind=real32), parameter :: learning_rate = 0.1
\end{verbatim}

\begin{itemize}
\item \textbf{\texttt{num\_iterations}}: The number of training iterations (epochs).
\item \textbf{\texttt{test\_size}}: The number of data points for both testing and each training mini-batch (30).
\item \textbf{\texttt{learning\_rate}}: The initial learning rate (0.1) for the \texttt{sgd} optimizer.
\end{itemize}


\bigskip
\centerline{\rule{13cm}{0.4pt}}
\bigskip

\subparagraph{3. \textbf{Neural Network Dimensions and Data Arrays}}

\begin{verbatim}
integer, parameter :: num_inputs = 1
  integer, parameter :: num_outputs = 1
  integer, parameter :: num_neurons_first_layer = 10
  integer, parameter :: num_neurons_second_layer = 10
\end{verbatim}

\begin{itemize}
\item \textbf{\texttt{num\_inputs}}: The network expects 1D input.
\item \textbf{\texttt{num\_outputs}}: The network outputs 1 value.
\item \textbf{\texttt{num\_neurons\_first\_layer}, \texttt{num\_neurons\_second\_layer}}: Two hidden layers with 10 neurons each.
\end{itemize}

\begin{verbatim}
real(kind=real32), dimension(test_size) :: x_test, y_test, x_train, y_train, y_pred
  real, dimension(1) :: x_train_arr_temp
  real, dimension(1) :: y_train_arr_temp
  real, dimension(1) :: x_test_arr_temp
  real, dimension(1) :: y_pred_arr_temp
\end{verbatim}

\begin{enumerate}
\item \textbf{\texttt{x\_test, y\_test}}: Arrays of size \texttt{test\_size} to store the input and target for the sine wave \textit{test set}.
\item \textbf{\texttt{x\_train, y\_train}}: Arrays of size \texttt{test\_size} for the random training mini-batch each iteration.
\item \textbf{\texttt{y\_pred}}: Predictions made on the test data, also size \texttt{test\_size}.
\item \textbf{\texttt{x\_train\_arr\_temp, y\_train\_arr\_temp, x\_test\_arr\_temp, y\_pred\_arr\_temp}}: All \texttt{dimension(1)}.

\begin{itemize}
\item These hold \textbf{single-sample} data for the neural network calls. The library expects arrays, even for single-value input.
\item For instance, \texttt{x\_train(j)} is stored in \texttt{x\_train\_arr\_temp(1)} before calling \texttt{forward}.
\end{itemize}
\end{enumerate}

\begin{verbatim}
real(kind=real32), dimension(num_iterations) :: mean_squared_error
  integer :: i, j

  real(kind=real32) :: x_train_temp
\end{verbatim}

\begin{itemize}
\item \textbf{\texttt{mean\_squared\_error}}: An array storing MSE for each of the \texttt{num\_iterations}.
\item \textbf{\texttt{x\_train\_temp}}: A random value used to generate the training data.
\end{itemize}


\bigskip
\centerline{\rule{13cm}{0.4pt}}
\bigskip

\subparagraph{3.1 \textbf{CSV File Initialization}}

\begin{verbatim}
call sine_NN_data_file%initialize(verbose=.true.)
  ...
  call sine_NN_data_file%open('sine_NN_data.csv', n_cols=7, status_ok=status_ok)
  if (.not. status_ok) then
    ...
  end if
  ...
  call sine_NN_data_file%add(['Iteration', 'x_test___', 'y_test___', &
                              'x_train__', 'y_train__', 'y_pred___', 'MSE______'])
  call sine_NN_data_file%next_row()
\end{verbatim}

\begin{enumerate}
\item \textbf{\texttt{initialize}}: Prepares the CSV object.
\item \textbf{\texttt{open}}: Opens a file named \texttt{sine\_NN\_data.csv}. The code also specifies \texttt{n\_cols=7}, meaning each row will eventually have up to 7 columns.
\item \textbf{Header}: \texttt{add([...])} writes a header row with column labels, then \texttt{next\_row()} finalizes that line.
\end{enumerate}


\bigskip
\centerline{\rule{13cm}{0.4pt}}
\bigskip

\subparagraph{4. \textbf{Creating Test Data}}

\begin{verbatim}
x_test = [((j - 1) * 2.0d0 * pi / test_size, j = 1, test_size)]
  y_test = (sin(x_test) + 1.0d0) / 2.0d0
\end{verbatim}

\begin{itemize}
\item For each \texttt{j} in \texttt{[1..test\_size]}, compute $\displaystyle x\_test(j) = \frac{(j - 1)\times 2\pi}{\text{test\_size}}$.

\begin{itemize}
\item This distributes 30 points evenly between \texttt{0} and \texttt{2$\pi$}.
\end{itemize}


\item The target is \texttt{y\_test(j) = (sin(x\_test(j)) + 1) / 2}, mapping the sine wave from \texttt{[-1, +1]} to \texttt{[0, 1]}.
\end{itemize}


\bigskip
\centerline{\rule{13cm}{0.4pt}}
\bigskip

\subparagraph{5. \textbf{Constructing the Neural Network}}

\begin{verbatim}
net = network([ &
    input(num_inputs), &
    dense(num_neurons_first_layer), &
    dense(num_neurons_second_layer), &
    dense(num_outputs) &
  ])
\end{verbatim}

\begin{itemize}
\item \textbf{\texttt{network([...])}}: Creates a layered neural network object with the following sequence:

\begin{enumerate}
\item \textbf{\texttt{input(num\_inputs)}}: 1D input layer.
\item \textbf{\texttt{dense(num\_neurons\_first\_layer)}}: A fully connected layer with 10 neurons.
\item \textbf{\texttt{dense(num\_neurons\_second\_layer)}}: Another hidden layer with 10 neurons.
\item \textbf{\texttt{dense(num\_outputs)}}: Final layer that outputs a single value.
\end{enumerate}
\end{itemize}

\begin{verbatim}
call net % print_info()
\end{verbatim}

\begin{itemize}
\item Prints summary info about the network (number of parameters, layer shapes, etc.).
\end{itemize}


\bigskip
\centerline{\rule{13cm}{0.4pt}}
\bigskip

\subparagraph{6. \textbf{Training Loop}}

\begin{verbatim}
do i = 1, num_iterations
\end{verbatim}

A loop over the total epochs (\texttt{num\_iterations} = 100,000). Each iteration does:

\subparagraph{6.1 \textbf{Mini-Batch Data Generation}}

\begin{verbatim}
do j = 1, test_size
      call random_number(x_train_temp)
      x_train(j) = x_train_temp * 2.0d0 * pi
      y_train(j) = (sin(x_train(j)) + 1.0d0) / 2.0d0
\end{verbatim}

\begin{itemize}
\item For each of the \texttt{test\_size} samples in the batch:

\begin{enumerate}
\item Generate a random number \texttt{x\_train\_temp} in [0, 1).
\item Scale it to \texttt{[0, 2$\pi$]}.
\item Compute \texttt{y\_train(j)} as \texttt{(sin(x\_train(j)) + 1.0)/2.0}, so the sine wave is in [0, 1].
\end{enumerate}
\end{itemize}

\subparagraph{6.2 \textbf{Per-Sample Forward/Backward}}

\begin{verbatim}
x_train_arr_temp = x_train(j)
      y_train_arr_temp = y_train(j)

      call net % forward(x_train_arr_temp)
      call net % backward(y_train_arr_temp)
      call net % update(optimizer=sgd(learning_rate=learning_rate))
\end{verbatim}

\begin{itemize}
\item \textbf{\texttt{x\_train\_arr\_temp}} and \textbf{\texttt{y\_train\_arr\_temp}} are 1-element arrays. Some neural net frameworks require arrays even for single-value inputs.
\item \textbf{\texttt{forward(...)}} passes the single sample through the network, producing internal activations.
\item \textbf{\texttt{backward(...)}} uses the single-sample label \texttt{y\_train\_arr\_temp} to compute gradients (loss derivative w.r.t. weights).
\item \textbf{\texttt{update(...)}} applies \textbf{Stochastic Gradient Descent} with \texttt{learning\_rate=0.1} to update network weights.
\end{itemize}

\textbf{Important}: This code does a per-sample update inside the loop over \texttt{j}. This is effectively ``online'' or ``sample-by-sample'' training, repeated \texttt{test\_size} times per iteration.

\subparagraph{6.3 \textbf{Prediction on the Test Sample}}

\begin{verbatim}
x_test_arr_temp = x_test(j)
      y_pred_arr_temp = net % predict(x_test_arr_temp)
      y_pred(j) = y_pred_arr_temp(1)
    end do
\end{verbatim}

\begin{itemize}
\item Takes the test input \texttt{x\_test(j)}, passes it through the network to get \texttt{y\_pred\_arr\_temp} (again a 1-element array).
\item Store \texttt{y\_pred(j) = y\_pred\_arr\_temp(1)}.
\end{itemize}

After this inner loop, we have:

\begin{itemize}
\item A mini-batch of \texttt{test\_size} random training samples used for weight updates.
\item \textbf{\texttt{y\_pred}} for each of the 30 test points in \texttt{x\_test}.
\end{itemize}


\bigskip
\centerline{\rule{13cm}{0.4pt}}
\bigskip

\subparagraph{6.4 \textbf{Compute Mean Squared Error}}

\begin{verbatim}
mean_squared_error(i) = sum((y_pred - y_test)**2) / test_size
\end{verbatim}

\begin{itemize}
\item Compare the network's predictions \texttt{y\_pred} with the true test values \texttt{y\_test}.
\item The MSE for iteration \texttt{i} is the average of squared differences.
\end{itemize}


\bigskip
\centerline{\rule{13cm}{0.4pt}}
\bigskip

\subparagraph{6.5 \textbf{Logging Each Sample to CSV}}

\begin{verbatim}
do j = 1, test_size
      call sine_NN_data_file%add([i], int_fmt='(i6)')
      call sine_NN_data_file%add([ &
        x_test(j), & 
        y_test(j), &
        x_train(j), &
        y_train(j), &
        y_pred(j), &
        mean_squared_error(i)], &
        real_fmt='(f9.6)')
      call sine_NN_data_file%next_row()
    end do
\end{verbatim}

\begin{itemize}
\item We output \textbf{50 rows} per iteration. Each row records:

\begin{enumerate}
\item \textbf{Iteration} number \texttt{i}.
\item \textbf{\texttt{x\_test(j)}}: The j-th test input.
\item \textbf{\texttt{y\_test(j)}}: The sine wave's true value at \texttt{x\_test(j)}.
\item \textbf{\texttt{x\_train(j)}}: The j-th random training sample.
\item \textbf{\texttt{y\_train(j)}}: The training label for that sample.
\item \textbf{\texttt{y\_pred(j)}}: The network's prediction for \texttt{x\_test(j)}.
\item \textbf{\texttt{mean\_squared\_error(i)}}: The iteration's MSE.
\end{enumerate}
\end{itemize}

Hence, each iteration appends 50 lines to the CSV file, for a total of: $50 \times \text{num_iterations}$ lines.


\bigskip
\centerline{\rule{13cm}{0.4pt}}
\bigskip

\subparagraph{6.6 \textbf{Occasional Print of MSE}}

\begin{verbatim}
if (mod(i, (num_iterations/10)) == 0) then
      print '("Iteration: ", I6, " | MSE: ", F9.6)', i, mean_squared_error(i)
    end if
\end{verbatim}

\begin{itemize}
\item Every 1/10th of \texttt{num\_iterations} (in this case, every 10000 iterations if \texttt{num\_iterations=100000}), print the iteration number and current MSE to the console.
\end{itemize}


\bigskip
\centerline{\rule{13cm}{0.4pt}}
\bigskip

\subparagraph{7. \textbf{Closing Steps}}

\begin{verbatim}
print *, '[*] Training complete!'
  print *, '[*] Saving data to the CSV file...'
  ...
  call sine_NN_data_file%close(status_ok=status_ok)
  if (.not. status_ok) then
    print *, '[!] Error closing the CSV file...'
    stop
  end if
end program sine
\end{verbatim}

\begin{itemize}
\item Signals the end of training and closes the CSV file, ensuring everything is written to disk properly.
\end{itemize}


\bigskip
\centerline{\rule{13cm}{0.4pt}}
\bigskip

\subparagraph{8. \textbf{Summary of the Workflow}}

\begin{enumerate}
\item \textbf{Initialize}: Creates a CSV object, sets up the neural network, and preps hyperparameters.
\item \textbf{Generate} a small, fixed test set (\texttt{x\_test, y\_test}) spanning 0 to $2\pi$.
\item \textbf{Create} the network with 1 input neuron, two hidden layers of 10 each, and 1 output.
\item \textbf{Training} (outer loop over \texttt{i}):

\begin{itemize}
\item Build a random mini-batch of \texttt{test\_size} samples (\texttt{x\_train, y\_train}).
\item For each sample:

\begin{itemize}
\item Convert the single value to a 1-element array, do \texttt{forward/backward/update}.
\item Predict on the corresponding test sample \texttt{x\_test(j)}.
\end{itemize}


\item Compute MSE over all 50 test points.
\item Write iteration data (50 rows) into a CSV file.
\item Occasionally print progress.
\end{itemize}


\item \textbf{Finish}: Print completion message, close the CSV file.
\end{enumerate}

\textbf{Key Observations}:

\begin{itemize}
\item The code mixes ``training data'' that's random each iteration with a fixed ``test set'' (\texttt{x\_test}). This is somewhat unusual---often you keep a consistent training set or you randomize from a larger pool. But it works here as a demonstration of incremental training for the sine function.
\item The network is updated on a per-sample basis, repeated 50 times per iteration. That's effectively a ``mini-batch'' of size 1 repeated 50 times.
\item The final CSV ends up quite large: \texttt{num\_iterations $\times$ test\_size} lines, with 7 columns each.
\end{itemize}

This design shows how to integrate data generation, single-sample training steps, and CSV logging within a single Fortran program using a custom \texttt{network} library.

\subparagraph{Program Code}

\subparagraph{section\_examples\_sine\_wave.f90}

\begin{verbatim}
program sine
  use csv_module
  use nf, only: dense, input, network, sgd
  use iso_fortran_env, only: real32

  implicit none

  real, parameter :: pi = 4 * atan(1.)
  
  ! CSV file objects for the training, testing, and prediction data
  type(csv_file) :: sine_NN_data_file
  type(network) :: net

  ! Status of the CSV file operations
  logical :: status_ok
 
  ! Neural Network Training parameters
  integer, parameter :: num_iterations = 100000
  integer, parameter :: test_size = 50
  real(kind=real32), parameter :: learning_rate = 0.1
  
  ! Neural Network (Fully Connected) Parameters
  integer, parameter :: num_inputs = 1
  integer, parameter :: num_outputs = 1
  integer, parameter :: num_neurons_first_layer = 10
  integer, parameter :: num_neurons_second_layer = 10

  ! Training, Testing, and Prediction Data
  real(kind=real32), dimension(test_size) :: x_test, y_test, x_train, y_train, y_pred
  real, dimension(1) :: x_train_arr_temp
  real, dimension(1) :: y_train_arr_temp
  real, dimension(1) :: x_test_arr_temp
  real, dimension(1) :: y_pred_arr_temp
  
  ! Mean Squared Error
  real(kind=real32), dimension(num_iterations) :: mean_squared_error

  ! Loop variables
  integer :: i, j

  ! Temporary variables
  real(kind=real32) :: x_train_temp

  ! Initialize the CSV file
  call sine_NN_data_file%initialize(verbose=.true.)
  
  ! Open the CSV file
  print *, '[*] Opening the CSV file...'
  call sine_NN_data_file%open('sine_NN_data.csv', n_cols=7, status_ok=status_ok)
  if (.not. status_ok) then
    print *, '[!] Error opening the CSV file...'
    stop
  end if

  ! Add header to the CSV file
  print *, '[*] Adding header to the CSV file...'
  call sine_NN_data_file%add(['Iteration', 'x_test___', 'y_test___', 'x_train__', 'y_train__', 'y_pred___', 'MSE______'])
  call sine_NN_data_file%next_row()
  
  print '("[*] Creating Sine Wave Testing Data")'

  ! Create the Sine Wave Testing Data
  x_test = [((j - 1) * 2.0d0 * pi / test_size, j = 1, test_size)]
  y_test = (sin(x_test) + 1.0d0) / 2.0d0

  ! Create the Neural Network
  print *, '[*] Creating the Neural Network...'
  print '(60("="))'
  net = network([ &
    input(num_inputs), &
    dense(num_neurons_first_layer), &
    dense(num_neurons_second_layer), &
    dense(num_outputs) &
  ])

  ! Print the Neural Network information
  call net % print_info()
  print *

  ! Train the Neural Network
  print *, '[*] Training the Neural Network...'
  print '(60("="))'

  ! Loop over the number of iterations (epochs)
  do i = 1, num_iterations

    ! Train the Neural Network on the mini - batch of training data
    do j = 1, test_size
      ! Generate a random mini - batch of training data
      call random_number(x_train_temp)
      x_train(j) = x_train_temp * 2.0d0 * pi
      y_train(j) = (sin(x_train(j)) + 1.0d0) / 2.0d0

      ! Forward / backward pass and update the weights
      x_train_arr_temp = x_train(j)
      y_train_arr_temp = y_train(j)
      call net % forward(x_train_arr_temp)
      call net % backward(y_train_arr_temp)
      call net % update(optimizer=sgd(learning_rate=learning_rate))

      ! Evaluate the Neural Network on the testing data
      x_test_arr_temp = x_test(j)
      y_pred_arr_temp = net % predict(x_test_arr_temp)
      y_pred(j) = y_pred_arr_temp(1)
    end do
    
    ! Calculate the mean squared error
    mean_squared_error(i) = sum((y_pred - y_test)**2) / test_size

    do j = 1, test_size
      ! Save the iteration number, testing, training, prediction and mean squared error data to the CSV file
      ! For each interation there will be test_size rows of data of testing, training, prediction and mean squared error
      call sine_NN_data_file%add([i], int_fmt='(i6)')
      call sine_NN_data_file%add([ &
        x_test(j), & 
        y_test(j), &
        x_train(j), &
        y_train(j), &
        y_pred(j), &
        mean_squared_error(i)], &
        real_fmt='(f9.6)')
      call sine_NN_data_file%next_row()
    end do

    ! Print the MSE every (num_iterations/10) iterations
    if (mod(i, (num_iterations/10)) == 0) then
      print '("Iteration: ", I6, " | MSE: ", F9.6)', &
        i, mean_squared_error(i)
    end if
  
  end do

  print *, '[*] Training complete!'
  print *, '[*] Saving data to the CSV file...'

  ! Close the CSV file
  print *, '[*] Closing the CSV file...'
  call sine_NN_data_file%close(status_ok=status_ok)
  if (.not. status_ok) then
    print *, '[!] Error closing the CSV file...'
    stop
  end if
  
end program sine
\end{verbatim}

The above program is compiled and run using Fortran Package Manager (fpm).  The following FPM configuration file (fpm.toml) was used:

\begin{verbatim}
name = "Section_Examples_Sine_Wave"

[build]
auto -executables = true
auto -tests = true
auto -examples = true

[install]
library = false

[dependencies]
csv -fortran = { git="https://github.com/jacobwilliams/csv -fortran.git" }
neural -fortran = { git="https://github.com/modern -fortran/neural -fortran.git" }

[[executable]]
name="Section_Examples_Sine_Wave"
source -dir="app"
main="section_examples_sine_wave.f90"
\end{verbatim}

\subparagraph{Build the Program using FPM (Fortran Package Manager)}

\begin{verbatim}
import os
root_dir = ""
root_dir = os.getcwd()
\end{verbatim}

\begin{verbatim}
code_dir = root_dir + "/" + "Fortran_Code/Section_Examples_Sine_Wave"
\end{verbatim}

\begin{verbatim}
os.chdir(code_dir)
\end{verbatim}

\begin{verbatim}
build_status = os.system("fpm build 2>/dev/null")
\end{verbatim}

\subparagraph{Run the Program using FPM (Fortran Package Manager)}

The program is run and the output is saved into a file named sine\_NN\_data.dat

\begin{verbatim}
exec_status = \
    os.system("fpm run 2>/dev/null")
\end{verbatim}

\begin{verbatim}
[*] Opening the CSV file...
 [*] Adding header to the CSV file...
[*] Creating Sine Wave Testing Data
 [*] Creating the Neural Network...
============================================================
Layer: input
 - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
Output shape: 1
Parameters: 0

Layer: dense
 - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
Input shape: 1
Output shape: 10
Parameters: 20
Activation: sigmoid

Layer: dense
 - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
Input shape: 10
Output shape: 10
Parameters: 110
Activation: sigmoid

Layer: dense
 - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
Input shape: 10
Output shape: 1
Parameters: 11
Activation: sigmoid


 [*] Training the Neural Network...
============================================================
Iteration:  10000 | MSE:  0.007093
Iteration:  20000 | MSE:  0.000202
Iteration:  30000 | MSE:  0.000148
Iteration:  40000 | MSE:  0.000123
Iteration:  50000 | MSE:  0.000105
Iteration:  60000 | MSE:  0.000094
Iteration:  70000 | MSE:  0.000087
Iteration:  80000 | MSE:  0.000081
Iteration:  90000 | MSE:  0.000073
Iteration: 100000 | MSE:  0.000065
 [*] Training complete!
 [*] Saving data to the CSV file...
 [*] Closing the CSV file...
\end{verbatim}

\subparagraph{Plot the Neural Network Predictions and Mean Squared Error}

\subparagraph{Read the Data from the CSV File}

\begin{verbatim}
import pandas as pd
import matplotlib.pyplot as plt
\end{verbatim}

\begin{verbatim}
csv_file = "sine_NN_data.csv"
df = pd.read_csv(csv_file, header=0, names=[
        "Iteration", "x_test", "y_test", "x_train", "y_train", "y_pred", "MSE"
    ])
df.info()
\end{verbatim}

\begin{verbatim}
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 5000000 entries, 0 to 4999999
Data columns (total 7 columns):
 #   Column     Dtype  
 - - -  - - - - - -     - - - - -  
 0   Iteration  int64  
 1   x_test     float64
 2   y_test     float64
 3   x_train    float64
 4   y_train    float64
 5   y_pred     float64
 6   MSE        float64
dtypes: float64(6), int64(1)
memory usage: 267.0 MB
\end{verbatim}

\subparagraph{Group and Average the Mean Squared Error by Iteration}

\begin{verbatim}
grouped = df.groupby("Iteration")
mse_by_iter = grouped["MSE"].mean()
overall_mse = mse_by_iter.mean()
print(f"Overall Mean Squared Error: {overall_mse}")
\end{verbatim}

\begin{verbatim}
Overall Mean Squared Error: 0.0014115740300000004
\end{verbatim}

\subparagraph{Plot the Neural Network Predictions vs True Sine Wave}

\begin{verbatim}
iteration_step = 5000 # Plot every iteration_step iteration
iteration_list = mse_by_iter.index.to_list()
iterations_to_plot = [1] + list(range(iteration_step, iteration_list[ -1]+1, iteration_step))
\end{verbatim}

\begin{verbatim}
plt.figure(figsize=(10, 6))
for iteration in iterations_to_plot:
    group_df = grouped.get_group(iteration)

    plt.plot(group_df["x_test"], group_df["y_test"], 'k - -', alpha=0.5,
                 label="True Sine Wave")
    plt.plot(group_df["x_test"], group_df["y_pred"], label=f"Iteration {iteration}")

plt.xlabel("x_test")
plt.ylabel("sine value")
plt.title(f"Predicted vs. True Sine Wave (plotting every {iteration_step} -th iteration)")
# Avoid repeated legend entries for the 'True Sine' if multiple lines are drawn
handles, labels = plt.gca().get_legend_handles_labels()
# Filter out duplicates by building an ordered dictionary
from collections import OrderedDict
by_label = OrderedDict(zip(labels, handles))
plt.legend(by_label.values(), by_label.keys(), loc="lower left", ncol=2)

plt.grid(True)
plt.tight_layout()
plt.show()
\end{verbatim}

\includegraphics[width=0.7\linewidth]{files/4e3dc9f42dbc67029e84725e5f51f7dc.png}

\subparagraph{Plot the Mean Squared Error vs Iteration}

\begin{verbatim}
plt.figure(figsize=(8, 6))
plt.plot(mse_by_iter.index[::1000], mse_by_iter.values[::1000], marker='o')
plt.title("Mean Squared Error vs. Iteration")
plt.xlabel("Iteration")
plt.ylabel("MSE")
plt.grid(True)
plt.tight_layout()
plt.yscale("log")
plt.show()
\end{verbatim}

\includegraphics[width=0.7\linewidth]{files/c95dca7eb02ee4b8dbbfd157ea0f2ff5.png}

\subparagraph{Conclusion}

We have successfully trained a neural network to approximate the sine function. The neural network was implemented in Fortran using the Neural Fortran library. The data was written to a CSV file using the CSV Fortran library. The neural network predictions and mean squared error were plotted using Python.

As can be seen from the plot, the neural network was able to approximate the sine function quite well. The mean squared error decreased over time as the neural network was trained. This example demonstrates how neural networks can be used to approximate complex functions such as the sine function.

As the iteration number reached 100,000, the neural network predictions closely matched the true sine wave function. The mean squared error decreased over time as the neural network was trained. This example demonstrates how neural networks can be used to approximate complex functions such as the sine function.

A great improvement on this code would be to implement the training loop using OpenMP.  This would allow the training loop to be parallelized and run on multiple cores.